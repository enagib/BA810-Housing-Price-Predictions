---
title: "Final Team Notebook"
author: "Fucheng Yao, Limei Huang, Eman Nagib, Kwangwoo Kim, Huaiping Wang"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(ggmap)
library(maps)
library(mapdata)
library(glmnet)
library(magrittr) 
library(ggthemes)
library(dummy)
library(randomForest)
library(gbm)
library(readxl)
library(rpart)
library(rpart.plot)
library(ipred)
library(GGally)
```

## Problem: 
> What is the housing price given a certain housing criteria?

### Project impact:
>For owners - understand the value of your property relative to other properties that offer the same features
>
>For potential investors - assert if the house’s value is reasonable etc.
>
>For potential buyers/brokers - comparing properties and finding the best value proposition for a certain criteria

## Preparing our dataset:

```{r}
###############################
# Preparing our dataset table
###############################

kc_housing <- read_csv("../kc_house_data.csv")

kc_houses <- kc_housing %>% 
  mutate(transaction_id = row_number()) %>% 
  rename("house_id" = id) %>% 
  select(transaction_id, everything()) %>% 
  separate(date, into = c("year_sold", "month_sold", "day_sold"), sep = "-") 

kc_houses$year_sold <- as.numeric(kc_houses$year_sold)
kc_houses$month_sold <- as.numeric(kc_houses$month_sold)
kc_houses$day_sold <- as.numeric(kc_houses$day_sold)

kc_houses <- kc_houses %>% filter(bedrooms != 33)

kc_houses <- kc_houses %>% 
  mutate(renovated = ifelse(yr_renovated >0, 1, 0)) %>% 
  select(everything(), -yr_renovated)

kc_houses <- kc_houses %>%
  mutate(lat_direction = ifelse(lat >=47.548,"N","S")) %>% 
  mutate(long_direction = ifelse(abs(long) >=121.9836,"W","E")) %>% 
  mutate(direction=paste(lat_direction,long_direction)) %>% 
  select(-c("long","lat","long_direction","lat_direction"))

kc_house_new <- kc_houses %>%
  mutate(SW =ifelse(direction =="S W",1,0)) %>%
  mutate(NW =ifelse(direction =="N W",1,0)) %>%
  mutate(NE =ifelse(direction =="N E",1,0)) %>%
  mutate(SE =ifelse(direction =="S E",1,0)) %>% 
  select(-c(direction))

kc_house_new <- kc_house_new %>% mutate(years_old = 2019 - yr_built)

kc_house_new$price <- kc_house_new$price/100000

ddzipcode <- read_excel("zipcode.xlsx") 
ddzipcode2 <- ddzipcode %>% select(Zipcode, City)

kc_house_new2 <- left_join(kc_house_new, ddzipcode2, by = c("zipcode" = "Zipcode")) 
kc_house_new3 <- kc_house_new2 %>% mutate(city = factor(City)) %>% select(-City)

city <- kc_house_new3 %>% select(city)
city2 <- dummy::dummy(city) 

kc_house_new_4 <- cbind(kc_house_new3, city2) 
kc_house_new_4 <- kc_house_new_4 %>% filter(bedrooms != 0)

# Table with directions 
kc_direction <- kc_house_new_4 %>% 
  select(-c(transaction_id,house_id, zipcode, 
            year_sold, month_sold, day_sold, yr_built, city, city_Auburn, city_Bellevue,     
            city_Black.Diamond, city_Bothell, city_Carnation, 
            city_Duvall, city_Enumclaw, city_Fall.City,
            city_Federal.Way, city_Issaquah, city_Kenmore,    
            city_Kent, city_Kirkland, city_Maple.Valley,
            city_Medina, city_Mercer.Island, city_North.Bend,
            city_Redmond, city_Renton, city_Sammamish,   
            city_Seattle, city_Snoqualmie, city_Vashon,    
            city_Woodinville))

#Table with citites
kc_cities <- kc_house_new_4 %>% 
  select(-c(transaction_id,house_id, zipcode, 
            year_sold, month_sold, day_sold, yr_built, SW, NW, NE, SE))

```

## Data overview:
>Data source: Kaggle - https://www.kaggle.com/harlfoxem/housesalesprediction
>
>>Our dataset constitute of `r ncol(kc_cities)` columns and `r nrow(kc_cities)`.
>
>house_id - Unique ID for each home sold
>
>Year_sold, month_sold, day_sold - Date of the home sale
>
>price - Price of each home sold
>
>bedrooms - Number of bedrooms
>
>bathrooms - Number of bathrooms, where .5 accounts for a room with a toilet but no shower
>
>sqft_living - Square footage of the apartments interior living space
>
>sqft_lot - Square footage of the land space
>
>floors - Number of floors
>
>waterfront - A dummy variable for whether the apartment was overlooking the waterfront or not
>
>view - An index from 0 to 4 of how good the view of the property was
>
>condition - An index from 1 to 5 on the condition of the apartment
>
>grade - An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.
>
>sqft_above - The square footage of the interior housing space that is above ground level
>
>sqft_basement - The square footage of the interior housing space that is below ground level
>
>yr_built - The year the house was initially built
>
>yr_renovated - The year of the house’s last renovation
>
>zipcode - What zip code area the house is in
>
>lat - Latitude
>
>long - Longitude
>
>sqft_living15 - The square footage of interior housing living space for the nearest 15 neighbors
>
>sqft_lot15 - The square footage of the land lots of the nearest 15 neighbors

## Descriptive Analyses - explorartory dataset visualizations:
```{r}
##########################################################################################
#Average House Price Based on Renovation and Grade
##########################################################################################
yao <- kc_house_new %>%
  group_by(renovated,grade) %>% 
  summarise(avg_house_price=mean(price)*1000)

reno <- yao[c(3:22),]

reno$renovated <- as.factor(reno$renovated)

format(reno$avg_house_price, scientific=FALSE)

ggplot(reno, aes(factor(grade), avg_house_price, fill = renovated,label=round(avg_house_price,0))) +
  geom_bar(position = "dodge", stat = "identity")  +
  ggtitle("Average House Price Based on Renovation and Grade") +
  ylab("House Price") +
  xlab("Real Estate Apprisal Grade")
```


```{r}
###############################################################################
# city vs. price
###############################################################################

city_price <- kc_house_new_4 %>% 
  group_by(city) %>% 
  summarize(avg_price = mean(price),
            max_price = max(price),
            min_price = min(price))

col<- c("darkblue" = "Max Price" , "royalblue" = "Average Price", "skyblue" = "Min Price" )
ggplot(city_price) +
  geom_col(aes(city, sort(max_price, decreasing =  T)), fill = "darkblue") +
  geom_col(aes(city, avg_price), fill = "royalblue") +
  geom_col(aes(city, min_price), fill = "skyblue") +
  ggtitle("Comparison of Cities") +
  ylab("Price") +
    scale_colour_manual(name="Line Color", values=c("darkblue", "royalblue", "skyblue"),
                      labels = c("Max Price" , "Average Price", "Min Price" ))+
  theme(axis.text.x = element_text(angle = 90)) +
  scale_y_continuous(breaks = seq(0, 80, 10)) 

```


```{r}
###############################################################################
# best correlations 
###############################################################################

#plot correlation
ggcorr(kc_house_new_4 %>% select(-c(transaction_id,house_id, zipcode, 
                                    year_sold, month_sold, day_sold, yr_built, city_Auburn, city_Bellevue,     
                                    city_Black.Diamond, city_Bothell, city_Carnation, 
                                    city_Duvall, city_Enumclaw, city_Fall.City,
                                    city_Federal.Way, city_Issaquah, city_Kenmore,    
                                    city_Kent, city_Kirkland, city_Maple.Valley,
                                    city_Medina, city_Mercer.Island, city_North.Bend,
                                    city_Redmond, city_Renton, city_Sammamish,   
                                    city_Seattle, city_Snoqualmie, city_Vashon,    
                                    city_Woodinville)), 
       name = "corr", label = TRUE, hjust = 1, 
       abel_size = 2.5, angle = -45, size = 3)
```


```{r}
#Linear Plot for attributes with high correlations with price

p1 <- ggplot(kc_house_new_4, aes(x = bathrooms, y = price)) +
  geom_point(color = 'navajowhite4', size = 0.5) +
  geom_smooth(method="lm", color = 'black', size = 0.8) +
  theme_linedraw() + 
  labs(x = 'Total Bathrooms', y = 'Price (USD)') + 
  scale_y_continuous(labels = scales::comma)

p2 <- ggplot(kc_house_new_4, aes(x = sqft_living, y = price)) +
  geom_point(color = 'navajowhite', size = 0.5) +
  geom_smooth(method="lm", color = 'black', size = 0.5) +
  theme_linedraw() + 
  labs(x = 'Living Area (sq.ft)', y = 'Price (USD)') + 
  scale_y_continuous(labels = scales::comma)

p3 <- ggplot(kc_house_new_4, aes(x = grade, y = price)) +
  geom_point(color = 'navajowhite1', size = 0.5) +
  geom_smooth(method="lm", color = 'black', size = 0.5) +
  theme_linedraw() + 
  labs(x = 'Grade', y = 'Price (USD)') + 
  scale_y_continuous(labels = scales::comma)

p4 <- ggplot(kc_house_new_4, aes(x = sqft_above, y = price)) +
  geom_point(color = 'navajowhite2', size = 0.5) +
  geom_smooth(method="lm", color = 'black', size = 0.5) +
  theme_linedraw() + 
  labs(x = 'House Area (sq.ft)', y = 'Price (USD)') + 
  scale_y_continuous(labels = scales::comma)

p5 <- ggplot(kc_house_new_4, aes(x = sqft_living15, y = price)) +
  geom_point(color = 'navajowhite3', size = 0.5) +
  geom_smooth(method="lm", color = 'black', size = 0.5) +
  theme_linedraw() + 
  labs(x = 'Living Area 2015 (sq.ft)', y = 'Price (USD)') + 
  scale_y_continuous(labels = scales::comma)

grid.arrange(p1,p2,p3,p4,p5, nrow = 3,
             top = "House Sales in King County, USA")
```
## Set formula for later predictions:
```{r}
#####################################################
# Set our formula to be used for predictions later
#####################################################

# Formula using the direction dataset
f <- as.formula(price ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + waterfront + 
                  view + condition + grade + sqft_above + sqft_basement + renovated +
                  sqft_living15 + sqft_lot15 + SW + NW + NE + SE, kc_train)

# Formula using the city dataset
f <- as.formula(price ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + waterfront + 
                  view + condition + grade + sqft_above + sqft_basement + renovated +
                  sqft_living15 + sqft_lot15 + city_Auburn + city_Bellevue + city_Black.Diamond +
                  city_Bothell + city_Carnation + city_Duvall + city_Enumclaw + city_Fall.City +
                  city_Federal.Way +city_Issaquah+ city_Kenmore + city_Kent + city_Kirkland + city_Maple.Valley +
                  city_Medina + city_Mercer.Island + city_North.Bend + city_Redmond + city_Renton + city_Sammamish +
                  city_Seattle + city_Snoqualmie + city_Vashon + city_Woodinville, kc_train)
```

## Splitting dataset for train and test:
```{r}
#####################################################
# Splitting our dataset into test and train
#####################################################
train <- round(0.8 * nrow(kc_cities))
test <- nrow(kc_cities) - train


# spliting data into train and test
set.seed(98765)
train_index <- sample(nrow(kc_cities), train) # get index for random rows

kc_train <- kc_cities[train_index,]
kc_test <- kc_cities[-train_index,]

dim(kc_train)
dim(kc_test)


train <- round(0.8 * nrow(kc_cities))
test <- nrow(kc_cities) - train

x1_train <- model.matrix(f, kc_train)[, -1]
x1_test <- model.matrix(f, kc_test)[, -1]

y_train <- kc_train$price
y_test <- kc_test$price
```


## 1st Method - Linear Regression:
```{r warning=F}
fit_lm <- lm(f, kc_train)

# compute train and test MSEs for the above model:
yhat_train_lm <- predict(fit_lm)
mse_train_lm <- mean((kc_train$price - yhat_train_lm)^2)

yhat_test_lm <- predict(fit_lm, kc_test)
mse_test_lm <- mean((kc_test$price - yhat_test_lm)^2)

#inspect the coefficients of the linear regression model:
coef(fit_lm)

# Print MSEs
mse_train_lm
mse_test_lm
```

## 2nd Method - Lasso:
```{r}
x_train <- model.matrix(f, kc_train) [ , -1] #Intercept added by default
x_test <- model.matrix(f, kc_test) [ , -1]

# Fit the lasso model
fit_lasso <- cv.glmnet(x_train, kc_train$price, alpha = 1, nfolds = 10)

# compute train/test MSEs
yhat_train_lasso <- predict(fit_lasso, x_train, s = fit_lasso$lambda.min)
mse_train_lasso <- mean((kc_train$y - yhat_train_lasso)^2)

yhat_test_lasso <- predict(fit_lasso, x_test, s = fit_lasso$lambda.min)
mse_test_lasso <- mean((kc_test$y - yhat_test_lasso)^2)

#inspect the coefficients 
coef(fit_lasso)

# Print MSEs
mse_train_lasso
mse_test_lasso
```

## 3rd Method - Ridge:
```{r}
x_train <- model.matrix(f, kc_train) [ , -1] #Intercept added by default
x_test <- model.matrix(f, kc_test) [ , -1]

# Fit the lasso model
fit_ridge <- cv.glmnet(x_train, kc_train$price, alpha = 0, nfolds = 10)

# compute train/test MSEs
yhat_train_ridge <- predict(fit_ridge, x_train, s = fit_ridge$lambda.min)
mse_train_ridge <- mean((kc_train$price - yhat_train_lasso)^2)

yhat_test_ridge <- predict(fit_ridge, x_test, s = fit_ridge$lambda.min)
mse_test_ridge <- mean((kc_test$price - yhat_test_lasso)^2)

#inspect the coefficients 
coef(fit_ridge)

# Print MSEs
mse_train_ridge
mse_test_ridge
```

## 4th Method - Elastic Net:
```{r}
x_train <- model.matrix(f, kc_train) [ , -1] #Intercept added by default
x_test <- model.matrix(f, kc_test) [ , -1]

# Fit the lasso model
fit_en <- cv.glmnet(x_train, kc_train$price, alpha = 0.5, nfolds = 10)

# compute train/test MSEs
yhat_train_en<- predict(fit_en, x_train, s = fit_en$lambda.min)
mse_train_en <- mean((kc_train$price - yhat_train_lasso)^2)

yhat_test_en <- predict(fit_en, x_test, s = fit_en$lambda.min)
mse_test_en <- mean((kc_test$price - yhat_test_lasso)^2)

#inspect the coefficients
coef(fit_en)

# Print MSEs
mse_train_en
mse_test_en

```

## 5th Method - Forward Selection:
```{r warning=F}
## loop 
xnames <- colnames(kc_train)
xnames <- xnames[!xnames %in% c("price","transaction_id","house_id", "zipcode", 
                                "year_sold", "month_sold", "day_sold", "yr_built", "city")]

fit_fw <- lm(price ~ 1, data = kc_train)

yhat_train <- predict(fit_fw, kc_train)
mse_train <- mean((kc_train$price - yhat_train) ^ 2)

yhat_test <- predict(fit_fw, kc_test)
mse_test <- mean((kc_test$price - yhat_test) ^ 2)

xname <- "intercept"

log_fw <- tibble(
  xname = xname,
  model = paste0(deparse(fit_fw$call), collapse = ""),
  mse_train = mse_train,
  mse_test = mse_test)

while (length(xnames) > 0) {
  best_mse_train <- NA
  best_mse_test <- NA
  best_fit_fw <- NA
  best_xname <- NA
  
  for (xname in xnames) {
    fit_fw_tmp <- update(fit_fw, as.formula(paste0(". ~ . + ", xname)))
    yhat_train_tmp <- predict(fit_fw_tmp, kc_train)
    mse_train_tmp <- mean((kc_train$price - yhat_train_tmp) ^ 2)
    yhat_test_tmp <- predict(fit_fw_tmp, kc_test)
    mse_test_tmp <- mean((kc_test$price - yhat_test_tmp) ^ 2)
    
    if (is.na(best_mse_test) | mse_test_tmp < best_mse_test) {
      best_xname <- xname
      best_fit_fw <- fit_fw_tmp
      best_mse_train <- mse_train_tmp
      best_mse_test <- mse_test_tmp
    }
  }
  log_fw <- log_fw %>% add_row(
    xname = best_xname,
    model = paste0(deparse(best_fit_fw$call), collapse = ""),
    mse_train = best_mse_train,
    mse_test = best_mse_test
  )
  fit_fw <- best_fit_fw
  xnames <- xnames[xnames!=best_xname]
}

# Print MSEs
best_mse_train
best_mse_test
```
```{r}
### ggplot 
ggplot(log_fw, aes(seq_along(xname), mse_test)) +
  geom_point() +
  geom_line() +
  geom_point(aes(y=mse_train), color="blue") +
  geom_line(aes(y=mse_train), color="blue") +
  ggtitle("Forward Selection")+
  scale_x_continuous("Variables", labels = log_fw$xname, breaks = seq_along(log_fw$xname)) +
  scale_y_continuous("MSE test") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


## 6th Method - Trees:
```{r}
fit.tree <- rpart(f,
                  kc_train,
                  control = rpart.control(cp = 0.001))

y_hat_train <- predict(fit.tree, kc_train)
tree_mse_train <- mean((y_hat_train - y_train)^2)

y_hat_test <- predict(fit.tree, kc_test)
tree_mse_test <- mean((y_hat_test - y_test)^2)

# Print MSEs
tree_mse_train 
tree_mse_test
```
```{r fig.height= 20}
par(xpd = T)
plot(fit.tree, compress=TRUE)
text(fit.tree, use.n=TRUE)
```

## 7th Method - Bagging:
```{r}
tree_bag <- bagging(f, data=kc_train, coob=TRUE)

y_hat_train <- predict(tree_bag, kc_train)
tree_mse_train_bag <- mean((y_hat_train - y_train)^2)

y_hat_test <- predict(tree_bag, kc_test)
tree_mse_test_bag <- mean((y_hat_test - y_test)^2)

# Print MSEs
tree_mse_train_bag 
tree_mse_test_bag
```

## 8th Method - Random Forest:
```{r}
xnames <- colnames(kc_cities)
xnames <- xnames[!xnames %in% c("price", "bedrooms", "city")]

loopformula <- " price ~ bedrooms"
for (xname in xnames) {
  loopformula <- paste(loopformula, "+", xname, sep = "")
}

f <- as.formula(loopformula)

fit_rf <- randomForest(f,
                       kc_train,
                       ntree = 10,
                       do.trace = F)
fit_rf 

y_hat_train_random <- predict(fit_rf, kc_train)
tree_mse_train_random <- mean((y_hat_train_random - y_train)^2)
y_hat_test_random <- predict(fit_rf, kc_test)
tree_mse_test_random <- mean((y_hat_test_random - y_test)^2)

# Print MSEs
tree_mse_train_random 
tree_mse_test_random
```
```{r fig.height= 8}
varImpPlot(fit_rf)
```


## 9th Method - Boosting:
```{r}
fit_btree <- gbm(f,
                 data = kc_train,
                 distribution = "gaussian",
                 n.trees = 1500,
                 interaction.depth = 4,
                 shrinkage = 0.001)

relative.influence(fit_btree)

y_hat_train_boosting <- predict(fit_btree, kc_train, n.trees = 100)
tree_mse_train_boosting <- mean((y_hat_train_boosting - y_train)^2)

y_hat_test_boosting <- predict(fit_btree, kc_test, n.trees = 100)
tree_mse_test_boosting <- mean((y_hat_test_boosting - y_test)^2)

# Print MSEs
tree_mse_train_boosting 
tree_mse_test_boosting

```

## Comparing Trees, Bagging, Random Forest, and Boosting test MSEs
```{r fig.width= 10}
#Generating a Prediction matrix for each Tree
n.trees = seq(from=1 ,to=1500, by=10)
predmatrix<-predict(fit_btree, kc_train, n.trees = n.trees)

#Calculating The Mean squared Test Error
#contains the Mean squared test error for each of the 100 trees averaged
train.error <- with(kc_train, apply( (predmatrix- y_train)^2,2,mean))

#Plotting the test errors
par(xpd=FALSE)
plot(n.trees , train.error , pch=19,col="blue",xlab="Number of Trees",
     ylab="Test MSE", main = "Perfomance of Boosting on Test Set VS. Other Methods",ylim = c(0, 15))

abline(h = min(tree_mse_test_random),col="maroon") #test.err is the test error of a Random forest fitted on same data
abline(h = min(tree_mse_test_bag),col="green")
abline(h = min(tree_mse_test),col="purple")
legend("topright",c("Minimum MSE test for Random Forests", "Minimum MSE test for Bagging",
                    "Minimum MSE test for Tree"),col= c("maroon", "yellow", "purple"),lty=1,lwd=1)
```

## Results - Method Comparison by MSE


## Challenge: Preparing our variables for use as predictors  
>
> It took multiple trials to ensure we our predictors are correctly set up to be used. 
> For instance we tried chaning the latitude and logtitude values to zipcode and then having zipcodes as dummy variables.
> We also tryed chaning the latitude and logtitude values to directions (SW, NW, NE, SE) and having those directions as dummy varaibles.
> Next we converted the latitude and logtitude values to cities in King county and had those cities as dummy variables.
> Ultimately, we decided to convert the lat and log variables to cities since that provided the lowest test MSE for our dataset.


## Conclusion:
>Random Forest is the most suitable statistcal method to be used to make predictions for our dataset since it provides the lowest test MSE in comparison to other methods. 
>According to the Random Forest method, the most important attributes to consider when predicting housing prices in King's county are the following 13 variables: 
>> `sqft_living`
>> `grade`
>> `sqft_living15`
>> `years_old`
>> `bathrooms`
>> `sqft_above`
>> `view`
>> `waterfront`
>> `sqft_lot15`
>> `sqft_lot`
>> `city_Bellevue`
>> `sqft_basement`
>> `city_Medina`
>> `city_Seattle`
>> `city_Federal.Way`
>> `bedrooms`
>> `condition`
>> `city_Kent`
>> `city_Mercer.Island`
>> `city_Auburn`
>> `city_Kirkland`
>> `floors`
>> `renovated`
>> `city_Redmond` 
>> `city_Renton`
>> `city_Maple.Valley`
>> `city_Sammamish`
>> `city_Issaquah`
>> `city_Enumclaw`
>> `city_Snoqualmie`
